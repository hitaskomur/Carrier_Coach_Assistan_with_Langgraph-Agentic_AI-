{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb8caa-dd57-49fb-a401-25ec08806deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict,List,Annotated,Dict\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langchain_core.messages import HumanMessage,AIMessage,SystemMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.tools import DuckDuckGoSearchResults #searching tools\n",
    "from langchain_groq import ChatGroq\n",
    "from IPython.display import display,Image, Markdown\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"your_api_key\"\n",
    "model=\"llama3-8b-8192\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c245b9fc-7917-4c7a-9f31-bc3b84f8cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=model,temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a86a77-9c54-4d7a-8e67-82ca1748b944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoachState(TypedDict):\n",
    "    query:str\n",
    "    category:str\n",
    "    response:str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a2607-0736-488e-95bd-86e452c688a0",
   "metadata": {},
   "source": [
    "First we are defining utilities we will require further\n",
    "ðŸ‘‰ trim_messages\n",
    "\n",
    "trim_conversation Function: This function limits the conversation history to the latest messages (up to 10), ensuring only recent and relevant messages are retained in the promp\n",
    "\n",
    "save_file Function: Saves data into a uniquely timestamped Markdown file in the Agent_output folder, creating the folder if it doesn't exst.\n",
    "\n",
    "show_md_file Function: Reads and displays the content of a Markdown file within the notebook, rendering it in Markdown form readabilityblity. lity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4dceef-bacd-4448-bfb1-8952303565df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_conversation(prompt):\n",
    "    \"\"\"Trims conversation history to retain only the latest messages within the limit.\"\"\"\n",
    "    max_messages = 10  # Limit the conversation history to the latest 10 messages\n",
    "    return trim_messages(\n",
    "        prompt,\n",
    "        max_tokens=max_messages,  # Specifies the maximum number of messages allowed\n",
    "        strategy=\"last\",  # Trimming strategy to keep the last messages\n",
    "        token_counter=len,  # Counts tokens/messages using the length of the list\n",
    "        start_on=\"human\",  # Start trimming when reaching the first human message\n",
    "        include_system=True,  # Include system messages in the trimmed history\n",
    "        allow_partial=False,  # Ensures only whole messages are included\n",
    "    )\n",
    "\n",
    "def save_file(data,filename):\n",
    "    \"\"\"Saves data to a markdown file with a timestamped filename.\"\"\"\n",
    "    folder_name = \"Agent_output\" #Folder to store output files\n",
    "    os.makedirs(folder_name,exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    filename = f\"{filename}_{timestamp}.md\"\n",
    "\n",
    "    file_path = os.path.join(folder_name,filename)\n",
    "\n",
    "    with open(file_path,\"w\",encoding=\"utf-8\") as file:\n",
    "        file.write(data)\n",
    "        print(f\"File '{file_path}' created succesfully.\")\n",
    "    return file_path\n",
    "\n",
    "def show_md_file(file_path):\n",
    "    \"\"\"Displays the content of a markdown file as Markdown in the notebook.\"\"\"\n",
    "    with open(file_path,\"r\",encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4123f1a-9aab-44cc-84a9-cb17a71787f2",
   "metadata": {},
   "source": [
    "### Now We will create class which will be Responsible for Learning(Tutorial and Q&A sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dfac9c-6e20-4600-a465-ffcedc90302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningResourcesAgent:\n",
    "    def __init__(self, prompt):\n",
    "        self.model = ChatGroq(model = model)\n",
    "        self.prompt = prompt\n",
    "        self.tools = [DuckDuckGoSearchResults()]\n",
    "\n",
    "    def TutorialAgent(self,user_input):\n",
    "        # Set up an agent with tool access and execute a tutorial-style response\n",
    "        model_with_tools = self.model.bind_tools(tools=self.tools)\n",
    "        agent = self.prompt | model_with_tools \n",
    "        response = agent.invoke({\"input\":user_input}).content\n",
    "\n",
    "        path = save_file(str(response).replace(\"```markdown\",\"\").strip(),\"Tutorial\")\n",
    "        print(f\"Tutorial saved to {path}\")\n",
    "        return path\n",
    "\n",
    "    def QueryBot(self, user_input):\n",
    "        # Initiates a Q&A loop for continuous interaction with the user\n",
    "        print(\"\\nStarting the Q&A session. Type 'exit' to end the session.\\n\")\n",
    "        record_QA_session = []\n",
    "        record_QA_session.append('User Query: %s \\n' % user_input)\n",
    "        self.prompt.append(HumanMessage(content=user_input))\n",
    "        while True:\n",
    "            # Trim conversation history to maintain prompt size\n",
    "            self.prompt = trim_conversation(self.prompt)\n",
    "            \n",
    "            # Generate a response from the AI model and update conversation history\n",
    "            response = self.model.invoke(self.prompt)\n",
    "            record_QA_session.append('\\nExpert Response: %s \\n' % response.content)\n",
    "            \n",
    "            self.prompt.append(AIMessage(content=response.content))\n",
    "            \n",
    "            # Display the AI's response and prompt for user input\n",
    "            print('*' * 50 + 'AGENT' + '*' * 50)\n",
    "            print(\"\\nEXPERT AGENT RESPONSE:\", response.content)\n",
    "            \n",
    "            print('*' * 50 + 'USER' + '*' * 50)\n",
    "            user_input = input(\"\\nYOUR QUERY: \")\n",
    "            record_QA_session.append('\\nUser Query: %s \\n' % response.content)\n",
    "            self.prompt.append(HumanMessage(content=user_input))\n",
    "            \n",
    "            # Exit the Q&A loop if the user types 'exit'\n",
    "            if user_input.lower() == \"exit\":\n",
    "                print(\"Ending the chat session.\")\n",
    "                path = save_file(''.join(record_QA_session),'Q&A_Doubt_Session')\n",
    "                print(f\"Q&A Session saved to {path}\")\n",
    "                return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a84ac-6021-466b-bf64-c1be4418b585",
   "metadata": {},
   "source": [
    "### Here we are creating Class for Interview handling(Interview Question Prep and MockInterview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f600f-8545-4e9d-bac8-c4a9ace5ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterviewAgent:\n",
    "    def __init__(self, prompt):\n",
    "        self.model = ChatGroq(model = model)\n",
    "        self.prompt = prompt\n",
    "        self.tools = [DuckDuckGoSearchResults()]\n",
    "\n",
    "    def Interview_questions(self,user_input):\n",
    "        # Holds the conversation history and cumulative questions and answers\n",
    "        chat_history = []\n",
    "        question_bank = \"\"\n",
    "        model_with_tools = self.model.bind_tools(self.tools)\n",
    "        self.agent = self.prompt | model_with_tools\n",
    "        while True:\n",
    "            print(\"\\nStarting the Interview question preparation. Type 'exit' to end the session. \\n\")\n",
    "            user_input = input(\"You\")\n",
    "            if user_input.lower() == \"exit\":\n",
    "                print(\"Ending the conversation. Goodbye!\")\n",
    "                break\n",
    "\n",
    "            # Generate a response to the user input and add it to questions_bank\n",
    "            response = self.agent.invoke({\"input\":user_input,\"chat_history\":chat_history}).content\n",
    "            question_bank += str(response).replace(\"```markdown\", \"\").strip() + \"\\n\"\n",
    "\n",
    "            chat_history.extend([HumanMessage(content=user_input),response])\n",
    "            if len(chat_history)>10:\n",
    "                chat_history = chat_history[-10]\n",
    "\n",
    "            \n",
    "\n",
    "        path = save_file(question_bank,\"Interview_questions\")\n",
    "        print(f\"Interview questions saved to {path}\")\n",
    "        return path\n",
    "\n",
    "\n",
    "    def Mock_Interview(self):\n",
    "        # Start a simulated mock interview session\n",
    "        print(\"\\nStarting the mock interview. Type 'exit' to end the session.\\n\")\n",
    "        \n",
    "        # Initialize with a starting message and store interview records\n",
    "        initial_message = 'I am ready for the interview.\\n'\n",
    "        interview_record = []\n",
    "        interview_record.append('Candidate: %s \\n' % initial_message)\n",
    "        self.prompt.append(HumanMessage(content=initial_message))\n",
    "        \n",
    "        while True:\n",
    "            # Trim conversation history if necessary to maintain prompt size\n",
    "            self.prompt = trim_conversation(self.prompt)\n",
    "            \n",
    "            # Generate a response using the chat model\n",
    "            response = self.model.invoke(self.prompt)\n",
    "            \n",
    "            # Add AI response to the conversation history\n",
    "            self.prompt.append(AIMessage(content=response.content))\n",
    "            \n",
    "            # Output the AI's response as the \"Interviewer\"\n",
    "            print(\"\\nInterviewer:\", response.content)\n",
    "            interview_record.append('\\nInterviewer: %s \\n' % response.content)\n",
    "            \n",
    "            # Get the user's response as \"Candidate\" input\n",
    "            user_input = input(\"\\nCandidate: \")\n",
    "            interview_record.append('\\nCandidate: %s \\n' % user_input)\n",
    "            \n",
    "            # Add user input to the conversation history\n",
    "            self.prompt.append(HumanMessage(content=user_input))\n",
    "            \n",
    "            # End the interview if the user types \"exit\"\n",
    "            if user_input.lower() == \"exit\":\n",
    "                print(\"Ending the interview session.\")\n",
    "                path = save_file(''.join(interview_record),'Mock_Interview')\n",
    "                print(f\"Mock Interview saved to {path}\")\n",
    "                return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88590588-f914-4cac-8fec-940196f215d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResumeMaker:\n",
    "    def __init__(self, prompt):\n",
    "        self.model = ChatGroq(model=model)\n",
    "        self.prompt = prompt\n",
    "        self.tools = [DuckDuckGoSearchResults()]\n",
    "\n",
    "        model_with_tools = self.model.bind_tools(self.tools)\n",
    "        self.agent = self.prompt | model_with_tools\n",
    "\n",
    "    def Create_Resume(self, user_input):\n",
    "        chat_history = []\n",
    "        # DÃ¶ngÃ¼den sonra kullanÄ±lacak son yanÄ±tÄ± saklamak iÃ§in bir deÄŸiÅŸken tanÄ±mlayalÄ±m.\n",
    "        final_response = None \n",
    "        \n",
    "        print(\"\\nResume making session starting. Type 'exit' if you want to end session.\\n\")\n",
    "        \n",
    "        while True:\n",
    "            # 1. Ã‡Ä±kÄ±ÅŸ komutunu kontrol et.\n",
    "            # 'or \"quit\"' hatasÄ± dÃ¼zeltildi.\n",
    "            if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "                print(\"Session ending. Good bye!\")\n",
    "                break\n",
    "            \n",
    "            # 2. Agent'Ä± Ã§alÄ±ÅŸtÄ±r ve yanÄ±tÄ± al.\n",
    "            # YanÄ±tÄ± 'response' deÄŸil, AIMessage nesnesi olarak alÄ±yoruz.\n",
    "            response_message = self.agent.invoke({\n",
    "                \"input\": user_input,\n",
    "                \"chat_history\": chat_history\n",
    "            })\n",
    "            \n",
    "            # YanÄ±tÄ±n iÃ§eriÄŸini alÄ±yoruz.\n",
    "            final_response = response_message.content\n",
    "            print(f\"AI: {final_response}\") # KullanÄ±cÄ±ya yanÄ±tÄ± gÃ¶sterelim.\n",
    "            \n",
    "            # 3. KonuÅŸma geÃ§miÅŸini gÃ¼ncelle.\n",
    "            chat_history.extend([\n",
    "                HumanMessage(content=user_input), \n",
    "                response_message # AIMessage'Ä±n tamamÄ±nÄ± eklemek daha doÄŸru.\n",
    "            ])\n",
    "\n",
    "            # GeÃ§miÅŸi belli bir uzunlukta tut.\n",
    "            if len(chat_history) > 10:\n",
    "                chat_history = chat_history[-10:]\n",
    "\n",
    "            # 4. KullanÄ±cÄ±dan yeni bir girdi al.\n",
    "            user_input = input(\"You: \")\n",
    "\n",
    "        # DÃ¶ngÃ¼ bittiÄŸinde, eÄŸer en az bir yanÄ±t Ã¼retilmiÅŸse dosyayÄ± kaydet.\n",
    "        if final_response:\n",
    "            path = save_file(str(final_response).replace(\"```markdown\", \"\").strip(), \"Resume\")\n",
    "            print(f\"CV ÅŸu yola kaydedildi: {path}\")\n",
    "            return path\n",
    "        else:\n",
    "            # KullanÄ±cÄ± hemen 'exit' yazarsa hiÃ§bir ÅŸey Ã¼retilmemiÅŸ olur.\n",
    "            print(\"No content created for record.\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c545f7-3bd8-4408-b921-df4bb4f59104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobSearch:\n",
    "    def __init__(self,prompt):\n",
    "        self.model = ChatGroq(model = model)\n",
    "        self.prompt = prompt\n",
    "        self.tools = DuckDuckGoSearchResults()\n",
    "\n",
    "    def find_jobs(self,user_input):\n",
    "        results = self.tools.invoke(user_input)\n",
    "        chain = self.prompt | self.model\n",
    "        jobs = chain.invoke({\"result\":results}).content\n",
    "\n",
    "        path = save_file(str(jobs).replace(\"```markdown\", \"\").strip(),\"Job_search\")\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06a463-4e1e-4f82-9937-a79fddc6ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(state: CoachState) -> CoachState:\n",
    "    \"\"\"Categorizes the user query into one of four main categories: Learn Generative AI Technology, Resume Making, Interview Preparation, or Job Search.\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Categorize the following customer query into one of these categories:\\n\"\n",
    "        \"1: Learn Generative AI Technology\\n\"\n",
    "        \"2: Resume Making\\n\"\n",
    "        \"3: Interview Preparation\\n\"\n",
    "        \"4: Job Search\\n\"\n",
    "        \"Give the number only as an output.\\n\\n\"\n",
    "        \"Examples:\\n\"\n",
    "        \"1. Query: 'What are the basics of generative AI, and how can I start learning it?' -> 1\\n\"\n",
    "        \"2. Query: 'Can you help me improve my resume for a tech position?' -> 2\\n\"\n",
    "        \"3. Query: 'What are some common questions asked in AI interviews?' -> 3\\n\"\n",
    "        \"4. Query: 'Are there any job openings for AI engineers?' -> 4\\n\\n\"\n",
    "        \"Now, categorize the following customer query:\\n\"\n",
    "        \"Query: {query}\"\n",
    "    )\n",
    "\n",
    "    # Creates a categorization chain and invokes it with the user's query to get the category\n",
    "    chain = prompt | llm \n",
    "    print('Categorizing the customer query...')\n",
    "    category = chain.invoke({\"query\": state[\"query\"]}).content\n",
    "    return {\"category\": category}\n",
    "\n",
    "def handle_learning_resouce(state:CoachState)->CoachState:\n",
    "    \"\"\"Determines if the query is related to Tutorial creation or general Questions on generative AI topics.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Categorize the following user query into one of these categories:\\n\\n\"\n",
    "        \"Categories:\\n\"\n",
    "        \"- Tutorial: For queries related to creating tutorials, blogs, or documentation on generative AI.\\n\"\n",
    "        \"- Question: For general queries asking about generative AI topics.\\n\"\n",
    "        \"- Default to Question if the query doesn't fit either of these categories.\\n\\n\"\n",
    "        \"Examples:\\n\"\n",
    "        \"1. User query: 'How to create a blog on prompt engineering for generative AI?' -> Category: Tutorial\\n\"\n",
    "        \"2. User query: 'Can you provide a step-by-step guide on fine-tuning a generative model?' -> Category: Tutorial\\n\"\n",
    "        \"3. User query: 'Provide me the documentation for Langchain?' -> Category: Tutorial\\n\"\n",
    "        \"4. User query: 'What are the main applications of generative AI?' -> Category: Question\\n\"\n",
    "        \"5. User query: 'Is there any generative AI course available?' -> Category: Question\\n\\n\"\n",
    "        \"Now, categorize the following user query:\\n\"\n",
    "        \"The user query is: {query}\\n\"\n",
    "    )  \n",
    "    chain = prompt | llm\n",
    "    print(\"Categorize the customer query futher...\")\n",
    "    response = chain.invoke({\"query\":state[\"query\"]}).content\n",
    "    return {\"category\":response}\n",
    "\n",
    "def handle_interview_preparation(state:CoachState)->CoachState:\n",
    "    \"\"\"Determines if the query is related to Mock Interviews or general Interview Questions.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Categorize the following user query into one of these categories:\\n\\n\"\n",
    "        \"Categories:\\n\"\n",
    "        \"- Mock: For requests related to mock interviews.\\n\"\n",
    "        \"- Question: For general queries asking about interview topics or preparation.\\n\"\n",
    "        \"- Default to Question if the query doesn't fit either of these categories.\\n\\n\"\n",
    "        \"Examples:\\n\"\n",
    "        \"1. User query: 'Can you conduct a mock interview with me for a Gen AI role?' -> Category: Mock\\n\"\n",
    "        \"2. User query: 'What topics should I prepare for an AI Engineer interview?' -> Category: Question\\n\"\n",
    "        \"3. User query: 'I need to practice interview focused on Gen AI.' -> Category: Mock\\n\"\n",
    "        \"4. User query: 'Can you list important coding topics for AI tech interviews?' -> Category: Question\\n\\n\"\n",
    "        \"Now, categorize the following user query:\\n\"\n",
    "        \"The user query is: {query}\\n\"\n",
    "    )\n",
    "\n",
    "    chain = prompt |llm\n",
    "    print(\"Categorizing the customer query futher...\")\n",
    "    response = chain.invoke({\"query\":state[\"query\"]}).content\n",
    "    return{\"category\":response}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c95856-b348-42d4-a28c-b2d4da5ee9d1",
   "metadata": {},
   "source": [
    "### Now we will create function for job search and Resume making\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a64ee3-5193-44da-ba70-6e74bf711cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_search(state:CoachState)->CoachState:\n",
    "    \"\"\"Provide a job search response based on user query requirements.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "    '''Your task is to refactor and make .md file for the this content which includes\n",
    "    the jobs available in the market. Refactor such that user can refer easily. Content: {result}'''\n",
    "    )\n",
    "    jobSearch = JobSearch(prompt)\n",
    "    state[\"query\"] = input('Please make sure to mention Job location you want,Job roles\\n')\n",
    "    path = jobSearch.find_jobs(state[\"query\"])\n",
    "    show_md_file(path)\n",
    "    return{\"response\":path}\n",
    "\n",
    "def handle_resume_making(state:CoachState)->CoachState:\n",
    "    \"\"\"Generate a customized resume based on user details for a tech role in AI and Generative AI.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", '''You are a skilled resume expert with extensive experience in crafting resumes tailored for tech roles, especially in AI and Generative AI. \n",
    "        Your task is to create a resume template for an AI Engineer specializing in Generative AI, incorporating trending keywords and technologies in the current job market. \n",
    "        Feel free to ask users for any necessary details such as skills, experience, or projects to complete the resume. \n",
    "        Try to ask details step by step and try to ask all details within 4 to 5 steps.\n",
    "        Ensure the final resume is in .md format.'''),\n",
    "       MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ])\n",
    "    resumeMaker = ResumeMaker(prompt)\n",
    "    path = resumeMaker.Create_Resume(state[\"query\"])\n",
    "    show_md_file(path)\n",
    "    return {\"response\":path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ddc242-e5eb-44d5-b0db-b9f0bb545a02",
   "metadata": {},
   "source": [
    "### Next we will create a function for Q&A query bot and Tutorial maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d886152-9551-46b5-ab08-913ffc27bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_query_bot(state:CoachState)->CoachState:\n",
    "    \"\"\"Provide detalied answers to user queries related to Generative AI.\"\"\"\n",
    "    system_message = '''You are an expert Generative AI Engineer with extensive experience in training and guiding others in AI engineering. \n",
    "    You have a strong track record of solving complex problems and addressing various challenges in AI. \n",
    "    Your role is to assist users by providing insightful solutions and expert advice on their queries.\n",
    "    Engage in a back-and-forth chat session to address user queries.'''\n",
    "    prompt = [SystemMessage(content=system_message)]\n",
    "\n",
    "    learning_agent = LearningResourcesAgent(prompt)\n",
    "\n",
    "    path = learning_agent.QueryBot(state[\"query\"])\n",
    "    show_md_file(path)\n",
    "    return{\"response\":path}\n",
    "\n",
    "def tutorial_agent(state:CoachState)->CoachState:\n",
    "    \"\"\"Generate a tutorial blog for Generative AI based on user requirements.\"\"\"\n",
    "    system_message = '''You are a knowledgeable assistant specializing as a Senior Generative AI Developer with extensive experience in both development and tutoring. \n",
    "         Additionally, you are an experienced blogger who creates tutorials focused on Generative AI.\n",
    "         Your task is to develop high-quality tutorials blogs in .md file with Coding example based on the user's requirements. \n",
    "         Ensure tutorial includes clear explanations, well-structured python code, comments, and fully functional code examples.\n",
    "         Provide resource reference links at the end of each tutorial for further learning.'''\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\",system_message),\n",
    "                                              (\"placeholder\",\"{chat_history}\"),\n",
    "                                               (\"human\",\"{input}\"),\n",
    "                                               (\"placeholder\",\"{agent_scratchpad}\"),\n",
    "                                              ])\n",
    "    learning_agent = LearningResourcesAgent(prompt)\n",
    "    path = learning_agent.TutorialAgent(state[\"query\"])\n",
    "    show_md_file(path)\n",
    "    return {\"response\":path}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b602e2f-82b5-4c0d-9ac5-8e0012edf6d0",
   "metadata": {},
   "source": [
    "### Finally we will create function Interview Question prep and Mock interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f14f4-fadc-497c-b234-ba55a21aaa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interview_topics_questions(state:CoachState)->CoachState:\n",
    "    \"\"\"Provide a curated list of interview questions related to Generative AI based on user input.\"\"\"\n",
    "    system_message = '''You are a good researcher in finding interview questions for Generative AI topics and jobs.\n",
    "                     Your task is to provide a list of interview questions for Generative AI topics and job based on user requirements.\n",
    "                     Provide top questions with references and links if possible. You may ask for clarification if needed.\n",
    "                     Generate a .md document containing the questions.'''\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",system_message),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\",\"{input}\"),\n",
    "        (\"placeholder\",\"{agent_scratchpad}\"),\n",
    "    ])\n",
    "    interview_agent = InterviewAgent(prompt)\n",
    "    path = interview_agent.Interview_questions(state[\"query\"])\n",
    "    show_md_file(path)\n",
    "    return{\"response\":path}\n",
    "\n",
    "def mock_interview(state:CoachState)->CoachState:\n",
    "    \"\"\"Conduct a mock interview for a Generative AI position, including evaluation at the end.\"\"\"\n",
    "    system_message = '''You are a Generative AI Interviewer. You have conducted numerous interviews for Generative AI roles.\n",
    "         Your task is to conduct a mock interview for a Generative AI position, engaging in a back-and-forth interview session.\n",
    "         The conversation should not exceed more than 15 to 20 minutes.\n",
    "         At the end of the interview, provide an evaluation for the candidate.'''\n",
    "    prompt = [SystemMessage(content = system_message)]\n",
    "    interview_agent = InterviewAgent(prompt)\n",
    "    path = interview_agent.Mock_Interview()\n",
    "    show_md_file(path)\n",
    "    return {\"response\":path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99801df-b599-4138-bc9f-e0f8d1c0e404",
   "metadata": {},
   "source": [
    "### Here, We are creating routing function which will be responsible for conditional edge to give direction after categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cd9e43-f95c-421a-9b14-8fb98ab141c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_query(state:CoachState):\n",
    "    \"\"\"Route the query based on its category to the appropriate handler.\"\"\"\n",
    "    if '1' in state[\"category\"]:\n",
    "        print('Category: handle_learning_resource')\n",
    "        return \"handle_learning_resource\"  # Directs queries about learning generative AI to the learning resource handler\n",
    "    elif '2' in state[\"category\"]:\n",
    "        print('Category: handle_resume_making')\n",
    "        return \"handle_resume_making\"  # Directs queries about resume making to the resume handler\n",
    "    elif '3' in state[\"category\"]:\n",
    "        print('Category: handle_interview_preparation')\n",
    "        return \"handle_interview_preparation\"  # Directs queries about interview preparation to the interview handler\n",
    "    elif '4' in state[\"category\"]:\n",
    "        print('Category: job_search')\n",
    "        return \"job_search\"  # Directs job search queries to the job search handler\n",
    "    else:\n",
    "        print(\"Please ask your question based on my description.\")\n",
    "        return False  # Returns False if the category does not match any predefined options\n",
    "\n",
    "def route_interview(state:CoachState)->str:\n",
    "    \"\"\"Route the query to the appropriate interview-related handler.\"\"\"\n",
    "    if 'Question'.lower() in state[\"category\"].lower():\n",
    "        print('Category: interview_topics_questions')\n",
    "        return \"interview_topics_questions\"  # Directs to the handler for interview topic questions\n",
    "    elif 'Mock'.lower() in state[\"category\"].lower():\n",
    "        print('Category: mock_interview')\n",
    "        return \"mock_interview\"  # Directs to the mock interview handler\n",
    "    else:\n",
    "        print('Category: mock_interview')\n",
    "        return \"mock_interview\"  # Defaults to mock interview if category does not clearly match\n",
    "\n",
    "\n",
    "def route_learning(state: CoachState):\n",
    "    \"\"\"Route the query based on the learning path category.\"\"\"\n",
    "    if 'Question'.lower() in state[\"category\"].lower():\n",
    "        print('Category: ask_query_bot')\n",
    "        return \"ask_query_bot\"  # Directs queries to the general question bot\n",
    "    elif 'Tutorial'.lower() in state[\"category\"].lower():\n",
    "        print('Category: tutorial_agent')\n",
    "        return \"tutorial_agent\"  # Directs queries to the tutorial creation agent\n",
    "    else:\n",
    "        print(\"Please ask your question based on my interview description.\")\n",
    "        return False  # Returns False if no clear category match is found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd5b9c-c889-4949-a156-eb4eb129c2e0",
   "metadata": {},
   "source": [
    "### Now all set lets create workflow graphs adding edges and nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f59a7-44f1-484d-ad38-9e12d88366d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(CoachState)\n",
    "\n",
    "workflow.add_node(\"categorize\",categorize)\n",
    "workflow.add_node(\"handle_learning_resource\",handle_learning_resouce)\n",
    "workflow.add_node(\"handle_resume_making\",handle_resume_making)\n",
    "workflow.add_node(\"handle_interview_preparation\",handle_interview_preparation)\n",
    "workflow.add_node(\"job_search\",job_search)\n",
    "workflow.add_node(\"mock_interview\",mock_interview)\n",
    "workflow.add_node(\"interview_topics_questions\",interview_topics_questions)\n",
    "workflow.add_node(\"tutorial_agent\",tutorial_agent)\n",
    "workflow.add_node(\"ask_query_bot\",ask_query_bot)\n",
    "\n",
    "workflow.add_edge(START,\"categorize\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"categorize\",\n",
    "    route_query,\n",
    "        {\n",
    "        \"handle_learning_resource\": \"handle_learning_resource\",\n",
    "        \"handle_resume_making\": \"handle_resume_making\",\n",
    "        \"handle_interview_preparation\": \"handle_interview_preparation\",\n",
    "        \"job_search\": \"job_search\"\n",
    "    }\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"handle_interview_preparation\",\n",
    "    route_interview,\n",
    "    {\n",
    "        \"mock_interview\": \"mock_interview\",\n",
    "        \"interview_topics_questions\": \"interview_topics_questions\",\n",
    "    }\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"handle_learning_resource\",\n",
    "    route_learning,\n",
    "    {\n",
    "        \"tutorial_agent\": \"tutorial_agent\",\n",
    "        \"ask_query_bot\": \"ask_query_bot\",\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"handle_resume_making\",END)\n",
    "workflow.add_edge(\"job_search\",END)\n",
    "workflow.add_edge(\"interview_topics_questions\", END)\n",
    "workflow.add_edge(\"mock_interview\", END)\n",
    "workflow.add_edge(\"ask_query_bot\", END)\n",
    "workflow.add_edge(\"tutorial_agent\", END)\n",
    "\n",
    "workflow.set_entry_point(\"categorize\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051633bf-5036-4b2b-8c0b-86206e5a8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a192396-871d-4807-8b4f-539518cf64f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8810e2d-220f-44ab-809a-feb257a6082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_user_query(query:str)->Dict[str,str]:\n",
    "    \"\"\"Process a user query through the LangGraph workflow.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's query\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: A dictionary containing the query's category and response\n",
    "    \"\"\"\n",
    "\n",
    "    results = app.invoke({\"query\":query})\n",
    "    return{\n",
    "        \"category\":results[\"category\"],\n",
    "        \"response\":results[\"response\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d526a-b8c0-4172-88a6-5eafd2e713e5",
   "metadata": {},
   "source": [
    "# ---------------------------Testing Different Scenarios------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d244b3-c31d-4e66-bc7b-5e4484c903ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I want to learn Langchain and langgraph.With usage and concept. Also give coding example implementation for both.Create tutorial for this.\"\n",
    "result = run_user_query(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7fd1c6-4be7-46e6-829c-2cc89a928fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I am confused between Langgraph and CrewAI when to use what for Agent Creation?\"\n",
    "result = run_user_query(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9697b089-9185-42f6-b2fa-e7f5846e3e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I want to discussion Interview question for Gen AI job roles.\"\n",
    "result = run_user_query(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013835ce-fdef-4847-a841-66a774014e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da5b4f2-a889-42b8-bdbd-915d8858abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I am confused between Langgraph and CrewAI when to use what for Agent Creation?\"\n",
    "result = run_user_query(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba6b99b-6b78-4f31-9885-9cdbf422672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I need mock interview to practice.\"\n",
    "result = run_user_query(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce1d7e-080c-4e7b-8897-f4903beec247",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you help me to modify my resume based on job description\"\n",
    "result = run_user_query(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea80289e-5f69-4352-8f30-81b29a997113",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I want to make resume for Gen AI roles job.\"\n",
    "result = run_user_query(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ad167-e572-47bf-a90a-ba3ed959acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I want to search jobs.\"\n",
    "\n",
    "result = run_user_query(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21561930-785d-49a9-99f5-ec3c595c333b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
